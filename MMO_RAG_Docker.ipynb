{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses Milvus through Docker Compose so you must have Docker installed to run this notebook (Milvus is spun up via `docker compose up -d` as shown in the block below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymilvus milvus langchain sentence-transformers tiktoken octoai-sdk\n",
    "# docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OCTOAI_API_TOKEN = os.environ.get(\"OCTOAI_API_TOKEN\")\n",
    "\n",
    "os.environ[\"OCTOAI_API_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n Instruction:\\n{question}\\n Response: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OctoAIEndpoint(\n",
    "    endpoint_url=\"https://text.octoai.run/v1/chat/completions\",\n",
    "    model_kwargs={\n",
    "        \"model\": \"mixtral-8x7b-instruct-fp16\",\n",
    "        \"max_tokens\": 128,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"temperature\": 0.01,\n",
    "        \"top_p\": 0.9,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Keep your responses limited to one short paragraph if possible.\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Exciting to see the visionary leadership of Elon Musk, driving innovation in electric vehicles, space exploration, and sustainable energy. His work with Tesla, SpaceX, and SolarCity is truly inspiring. #ElonMusk #Innovation #FutureTech\"\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me a tweet about elon musk\"\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.invoke(question)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the first prompt, you asked me to provide a brief introduction explaining that I am a helpful assistant and will keep responses limited to one short paragraph if possible.\n"
     ]
    }
   ],
   "source": [
    "question = \"What did I ask in the first prompt?\"\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.invoke(question)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OctoAIEmbeddings\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1248, which is longer than the specified 1024\n",
      "Created a chunk of size 1251, which is longer than the specified 1024\n",
      "Created a chunk of size 1094, which is longer than the specified 1024\n",
      "Created a chunk of size 2223, which is longer than the specified 1024\n",
      "Created a chunk of size 1212, which is longer than the specified 1024\n",
      "Created a chunk of size 1068, which is longer than the specified 1024\n",
      "Created a chunk of size 2034, which is longer than the specified 1024\n",
      "Created a chunk of size 1256, which is longer than the specified 1024\n",
      "Created a chunk of size 1089, which is longer than the specified 1024\n",
      "Created a chunk of size 1101, which is longer than the specified 1024\n",
      "Created a chunk of size 1539, which is longer than the specified 1024\n",
      "Created a chunk of size 1096, which is longer than the specified 1024\n",
      "Created a chunk of size 5866, which is longer than the specified 1024\n",
      "Created a chunk of size 1600, which is longer than the specified 1024\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    if (file != \".DS_Store\"):\n",
    "        with open(f\"./data/{file}\") as f:\n",
    "            file_text = f.read()\n",
    "\n",
    "        # text_splitter = CharacterTextSplitter(\n",
    "        #     separator=\"\\n\\n\",\n",
    "        #     chunk_size=1000,\n",
    "        #     chunk_overlap=200,\n",
    "        #     length_function=len,\n",
    "        #     is_separator_regex=False,\n",
    "        # )\n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name=\"cl100k_base\", chunk_size=1024, chunk_overlap=256, \n",
    "        )\n",
    "        texts = text_splitter.split_text(file_text)\n",
    "        for i, chunked_text in enumerate(texts):\n",
    "            file_texts.append(Document(page_content=chunked_text, \n",
    "                metadata={\"twitter_user\": file.split(\".\")[0], \"chunk_num\": i}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by the inference endpoint: Error: 400 {\"message\":\"Invalid request.\",\"errors\":{\"input.constrained-str\":[\"String should have at most 10240 characters\"],\"input.list[constrained-str]\":[\"Input should be a valid list\"]}}(request_id: )",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOctoAIClientError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/embeddings/octoai_embeddings.py:76\u001b[0m, in \u001b[0;36mOctoAIEmbeddings._compute_embeddings\u001b[0;34m(self, texts, instruction)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     resp_json \u001b[38;5;241m=\u001b[39m \u001b[43moctoai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_payload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp_json:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/octoai/client.py:188\u001b[0m, in \u001b[0;36mClient.infer\u001b[0;34m(self, endpoint_url, inputs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/octoai/client.py:164\u001b[0m, in \u001b[0;36mClient._error\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OctoAIClientError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOctoAIClientError\u001b[0m: Error: 400 {\"message\":\"Invalid request.\",\"errors\":{\"input.constrained-str\":[\"String should have at most 10240 characters\"],\"input.list[constrained-str]\":[\"Input should be a valid list\"]}}(request_id: )",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mMilvus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mport\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19530\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtwitter_user\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/vectorstores.py:528\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    527\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/milvus.py:991\u001b[0m, in \u001b[0;36mMilvus.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, collection_name, connection_args, consistency_level, index_params, search_params, drop_old, ids, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m     auto_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    980\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    981\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    982\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    990\u001b[0m )\n\u001b[0;32m--> 991\u001b[0m \u001b[43mvector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vector_db\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/milvus.py:533\u001b[0m, in \u001b[0;36mMilvus.add_texts\u001b[0;34m(self, texts, metadatas, timeout, batch_size, ids, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mencode()) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65_535\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ids\n\u001b[1;32m    530\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach id should be a string less than 65535 bytes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_func\u001b[38;5;241m.\u001b[39membed_query(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/embeddings/octoai_embeddings.py:95\u001b[0m, in \u001b[0;36mOctoAIEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute document embeddings using an OctoAI instruct model.\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), texts))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_instruction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/embeddings/octoai_embeddings.py:86\u001b[0m, in \u001b[0;36mOctoAIEmbeddings._compute_embeddings\u001b[0;34m(self, texts, instruction)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     embedding \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by the inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by the inference endpoint: Error: 400 {\"message\":\"Invalid request.\",\"errors\":{\"input.constrained-str\":[\"String should have at most 10240 characters\"],\"input.list[constrained-str]\":[\"Input should be a valid list\"]}}(request_id: )"
     ]
    }
   ],
   "source": [
    "vector_store = Milvus.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "    collection_name=\"twitter_user\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"It all begins today! I will see you at 11:00 A.M. for the swearing-in. THE MOVEMENT CONTINUES - THE WORK BEGINS!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Today we are not merely transferring power from one Administration to another\\t or from one party to another ‚Äì but we are transferring...\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n power from Washington\\t D.C. and giving it back to you\\t the American People. # InaugurationDay\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n What truly matters is not which party controls our government\\t but whether our government is controlled by the people.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n January 20th 2017\\t will be remembered as the day the people became the rulers of this nation again.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n The forgotten men and women of our country will be forgotten no longer. From this moment on\\t it‚Äôs going to be # AmericaFirst\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n We will bring back our jobs. We will bring back our borders. We will bring back our wealth - and we will bring back our dreams!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n We will follow two simple rules: BUY AMERICAN & HIRE AMERICAN! # InaugurationDay # MAGA\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n It is time to remember that...https://www.facebook.com/DonaldTrump/posts/10158510858055725 ‚Ä¶\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n So to all Americans\\t in every city near and far\\t small and large\\t from mountain to mountain...https://www.facebook.com/DonaldTrump/posts/10158510880180725 ‚Ä¶\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n TO ALL AMERICANS https://www.facebook.com/DonaldTrump/posts/10158510987625725 ‚Ä¶\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n THANK YOU for another wonderful evening in Washington\\t D.C. TOGETHER\\t we will MAKE AMERICA GREAT AGAINpic.twitter.com/V3aoj9RUh4\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n A fantastic day and evening in Washington D.C.Thank you to @ FoxNews and so many other news outlets for the GREAT reviews of the speech!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Had a great meeting at CIA Headquarters yesterday\\t packed house\\t paid great respect to Wall\\t long standing ovations\\t amazing people. WIN!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Watched protests yesterday but was under the impression that we just had an election! Why didn't these people vote? Celebs hurt cause badly.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Wow\\t television ratings just out: 31 million people watched the Inauguration\\t 11 million more than the very good ratings from 4 years ago!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Peaceful protests are a hallmark of our democracy. Even if I don't always agree\\t I recognize the rights of people to express their views.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Busy week planned with a heavy focus on jobs and national security. Top executives coming in at 9:00 A.M. to talk manufacturing in America.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Will be meeting at 9:00 with top automobile executives concerning jobs in America. I want new plants to be built here for cars sold here!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n A photo delivered yesterday that will be displayed in the upper/lower press hall. Thank you Abbas!pic.twitter.com/Uzp0ivvRp0\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Great meeting with automobile industry leaders at the @ WhiteHouse this morning. Together\\t we will # MAGA!pic.twitter.com/OXdiLOkGsZ\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Signing orders to move forward with the construction of the Keystone XL and Dakota Access pipelines in the Oval Office.pic.twitter.com/OErGmbBvYK ‚Äì bei The Oval Office\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Great meeting with Ford CEO Mark Fields and General Motors CEO Mary Barra at the @ WhiteHouse today.pic.twitter.com/T0eIgO6LP8\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Congratulations to @ FoxNews for being number one in inauguration ratings. They were many times higher than FAKE NEWS @ CNN - public is smart!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n If Chicago doesn't fix the horrible 'carnage' going on\\t 228 shootings in 2017 with 42 killings (up 24% from 2016)\\t I will send in the Feds!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Big day planned on NATIONAL SECURITY tomorrow. Among many other things\\t we will build the wall!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n I will be asking for a major investigation into VOTER FRAUD\\t including those registered to vote in two states\\t those who are illegal and....\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n even\\t those registered to vote who are dead (and many for a long time). Depending on results\\t we will strengthen up voting procedures!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n I will be making my Supreme Court pick on Thursday of next week.Thank you!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n I will be interviewed by @ DavidMuir tonight at 10 o'clock on @ ABC. Will be my first interview from the White House. Enjoy!pic.twitter.com/okiZ8ZeDgz ‚Äì bei The White House\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Beginning today\\t the United States of America gets back control of its borders. Full speech from today @ DHSgov: http://bit.ly/2jSZomhDHS pic.twitter.com/48iZam5Fai\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n As your President\\t I have no higher duty than to protect the lives of the American people.pic.twitter.com/o7YNUNwb8f\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n ' @ romoabcnews: . @ DavidMuir first @ POTUS interview since taking office. Tonight on @ ABCWorldNews @ ABC2020 tonight. pic.twitter.com/I4Vz1mRdBK'\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Interview with David Muir of @ ABC News in 10 minutes. Enjoy!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Ungrateful TRAITOR Chelsea Manning\\t who should never have been released from prison\\t is now calling President Obama a weak leader. Terrible!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n The U.S. has a 60 billion dollar trade deficit with Mexico. It has been a one-sided deal from the beginning of NAFTA with massive numbers...\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n of jobs and companies lost. If Mexico is unwilling to pay for the badly needed wall\\t then it would be better to cancel the upcoming meeting.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Spoke at the Congressional @ GOP Retreat in Philadelphia\\t PA. this afternoon w/ @ VP\\t @ SenateMajLdr\\t @ SpeakerRyan. Thank you for your support!pic.twitter.com/DEv2DnksDc\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Will be interviewed by @ SeanHannity on @ FoxNews at 10:00pm tonight. Enjoy!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Miami-Dade Mayor drops sanctuary policy. Right decision. Strong!http://www.miamiherald.com/news/local/community/miami-dade/article128984759.html ‚Ä¶\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Look forward to seeing final results of VoteStand. Gregg Phillips and crew say at least 3\\t000\\t000 votes were illegal. We must do better!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Mexico has taken advantage of the U.S. for long enough. Massive trade deficits & little help on the very weak border must change\\t NOW!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n The # MarchForLife is so important. To all of you marching --- you have my full support!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n . @ VP Mike Pence will be speaking at today's # MarchForLife -- You have our full support!https://twitter.com/vp/status/824757313441525761 ‚Ä¶\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Statement on International Holocaust Remembrance Day:https://www.whitehouse.gov/the-press-office/2017/01/27/statement-president-international-holocaust-remembrance-day ‚Ä¶\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Congratulations Secretary Mattis!pic.twitter.com/mkuhbegzqS ‚Äì bei The Pentagon\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n I promise that our administration will ALWAYS have your back. We will ALWAYS be with you!pic.twitter.com/D0aOWhOH4X\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n The failing @ nytimes has been wrong about me from the very beginning. Said I would lose the primaries\\t then the general election. FAKE NEWS!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Thr coverage about me in the @ nytimes and the @ washingtonpost gas been so false and angry that the times actually apologized to its.....\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n ...dwindling subscribers and readers.They got me wrong right from the beginning and still have not changed course\\t and never will. DISHONEST\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Today\\t we remember the crew of the Space Shuttle Challenger\\t 31 years later. # NeverForgetpic.twitter.com/OhshQsFRfl\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Somebody with aptitude and conviction should buy the FAKE NEWS and failing @ nytimes and either run it correctly or let it fold with dignity!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Our country needs strong borders and extreme vetting\\t NOW. Look what is happening all over Europe and\\t indeed\\t the world - a horrible mess!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n Christians in the Middle-East have been executed in large numbers. We cannot allow this horror to continue!\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n The joint statement of former presidential candidates John McCain & Lindsey Graham is wrong - they are sadly weak on immigration. The two...\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n ...Senators should focus their energies on ISIS\\t illegal immigration and border security instead of always looking to start World War III.\", metadata={'twitter_user': 'RealDonaldTrump', 'chunk_num': 0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided documents, there is no information about a specific policy that Donald Trump supports. However, the documents do mention that Donald Trump \"failed America\" according to Joe Biden, but the context of this statement is not explicitly linked to a particular policy.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What policy does donald trump support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this a bit more fun and showcase the multilingual capabilities of Mixtal which really outshine other open source models\n",
    "\n",
    "# Our Vector DB is populated with entries from english text - even the embedding model we're using here, GTE-Large\n",
    "# works best on english text. However Mixtral has good mutlilingual capabilities in French, German, Spanish and Italian.\n",
    "# So what we'll do is ask the assistant to only answer in french in the system and user prompt. RAG here is performed based on \n",
    "# english text, but upon producing the user response, the Mixtral LLM will generate tokens in a different language here (french)\n",
    "french_llm = OctoAIEndpoint(\n",
    "    endpoint_url=\"https://text.octoai.run/v1/chat/completions\",\n",
    "    model_kwargs={\n",
    "        \"model\": \"mixtral-8x7b-instruct-fp16\",\n",
    "        \"max_tokens\": 128,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant who responds in French and not in English.\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "french_template = \"\"\"Answer the question in French based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "french_prompt = PromptTemplate.from_template(french_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | french_prompt\n",
    "    | french_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_1 = french_chain.invoke(\"How big is the city of Seattle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' La question ne fournit pas assez de contexte pour une réponse précise '\n",
      " 'concernant la taille de la ville de Seattle. Cependant, je peux vous dire '\n",
      " \"que Seattle est la plus grande ville de l'État de Washington aux États-Unis \"\n",
      " \"et qu'elle s'étend sur une superficie d'environ 217 km².\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(fr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
